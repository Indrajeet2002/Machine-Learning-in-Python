{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Link to dataset: https://www.kaggle.com/datasets/salader/dogs-vs-cats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYmiOqfQMha7",
        "outputId": "7f6ddf01-4c53-414c-dc27-71e5be3c73f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpuNwVKGS7o3"
      },
      "source": [
        "Import tensorflow, and ImageDataGenerator from the keras library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cUBWxZ5QMpM6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UtBVB4EgM3PH",
        "outputId": "a6a9b74c-929d-422a-f5eb-d70748530b14"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.14.0'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFe-tuTNTG2a"
      },
      "source": [
        "Preprocess the train and test sets one at a time. For the training set, specify the rescale, shear_range, zoom_range and horizontal_flip parameters in ImageDataGenerator. For the test set only specify the rescale parameter. To perform image transformation, first feature scale the images, then transform the images, and connect the preprocessor instance to the datasets. This only needs to be done for the training set since the test does not need any transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEpINWGxHvnK",
        "outputId": "18ce0659-4c06-483b-a415-592905f11134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 20182 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/My Drive/CNN/train',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlRsWpcWKI1P",
        "outputId": "13b27928-e92c-4151-f9a0-fafe09263984"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/My Drive/CNN/test',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh4lI7w3WA8u"
      },
      "source": [
        "Use Sequential class to initialize the convolutional neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uGlefwccKU8h"
      },
      "outputs": [],
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAK6ecKgW-x-"
      },
      "source": [
        "Add a layer to the convolutional neural network, and specify the pool size and strides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GWhiIsftLDZK"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYiVldaMWSIP"
      },
      "source": [
        "Use the Conv2d method and specify the filters, which is the number of feature detectors, kernel_size which is the size of the feature detector, and the input_shape which is the RGB values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uWWY_ANkKbFw"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGHm8Fs5XSOc"
      },
      "source": [
        "Add more layers and specify the same parameters as above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0h7509fcKtcY"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KZQy0sPXeWC"
      },
      "source": [
        "Use the Flatten() method from the keras library to flatten the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DhLEn89pLLvn"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5ru2loCXoZl"
      },
      "source": [
        "In the Dense method the units are the number of neurons, and the activation function is relu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "W42Ua0heLL9f"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79gqB8EQX5JK"
      },
      "source": [
        "Since it is binary classification, specify the number of neurons as 1 and the activation function as sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zrkkhbRiLcOg"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB1JZvj_YR3z"
      },
      "source": [
        "Compile the CNN using the compile() method, and specify the optimizer, loss function and metrics parameters. The loss function calculates the difference between the predicted and actual values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5E-uWuWeLnHZ"
      },
      "outputs": [],
      "source": [
        "cnn.compile(optimizer =\"adam\" , loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcEQ11BSYc9Q"
      },
      "source": [
        "Use the fit method on the CNN model which was compiled earlier, and specify the epochs to be 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRQJmDyGLtrr",
        "outputId": "cd2e871b-6005-42d3-a406-5788fc96c30b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "631/631 [==============================] - 3905s 6s/step - loss: 0.6197 - accuracy: 0.6512 - val_loss: 0.6059 - val_accuracy: 0.6668\n",
            "Epoch 2/25\n",
            "631/631 [==============================] - 141s 223ms/step - loss: 0.5470 - accuracy: 0.7202 - val_loss: 0.5424 - val_accuracy: 0.7250\n",
            "Epoch 3/25\n",
            "631/631 [==============================] - 144s 228ms/step - loss: 0.5076 - accuracy: 0.7514 - val_loss: 0.4858 - val_accuracy: 0.7682\n",
            "Epoch 4/25\n",
            "631/631 [==============================] - 149s 236ms/step - loss: 0.4764 - accuracy: 0.7714 - val_loss: 0.4771 - val_accuracy: 0.7772\n",
            "Epoch 5/25\n",
            "631/631 [==============================] - 157s 248ms/step - loss: 0.4623 - accuracy: 0.7783 - val_loss: 0.4690 - val_accuracy: 0.7816\n",
            "Epoch 6/25\n",
            "631/631 [==============================] - 156s 247ms/step - loss: 0.4477 - accuracy: 0.7908 - val_loss: 0.4808 - val_accuracy: 0.7786\n",
            "Epoch 7/25\n",
            "631/631 [==============================] - 156s 247ms/step - loss: 0.4241 - accuracy: 0.8036 - val_loss: 0.4532 - val_accuracy: 0.7972\n",
            "Epoch 8/25\n",
            "631/631 [==============================] - 157s 250ms/step - loss: 0.4100 - accuracy: 0.8099 - val_loss: 0.4412 - val_accuracy: 0.7984\n",
            "Epoch 9/25\n",
            "631/631 [==============================] - 148s 234ms/step - loss: 0.3965 - accuracy: 0.8207 - val_loss: 0.4482 - val_accuracy: 0.8028\n",
            "Epoch 10/25\n",
            "631/631 [==============================] - 151s 239ms/step - loss: 0.3868 - accuracy: 0.8258 - val_loss: 0.4261 - val_accuracy: 0.8098\n",
            "Epoch 11/25\n",
            "631/631 [==============================] - 143s 227ms/step - loss: 0.3730 - accuracy: 0.8317 - val_loss: 0.4351 - val_accuracy: 0.8092\n",
            "Epoch 12/25\n",
            "631/631 [==============================] - 140s 221ms/step - loss: 0.3630 - accuracy: 0.8387 - val_loss: 0.4370 - val_accuracy: 0.8080\n",
            "Epoch 13/25\n",
            "631/631 [==============================] - 140s 222ms/step - loss: 0.3496 - accuracy: 0.8461 - val_loss: 0.4145 - val_accuracy: 0.8152\n",
            "Epoch 14/25\n",
            "631/631 [==============================] - 140s 222ms/step - loss: 0.3355 - accuracy: 0.8543 - val_loss: 0.4575 - val_accuracy: 0.8040\n",
            "Epoch 15/25\n",
            "631/631 [==============================] - 146s 231ms/step - loss: 0.3267 - accuracy: 0.8602 - val_loss: 0.4900 - val_accuracy: 0.7950\n",
            "Epoch 16/25\n",
            "631/631 [==============================] - 142s 225ms/step - loss: 0.3152 - accuracy: 0.8643 - val_loss: 0.4709 - val_accuracy: 0.8058\n",
            "Epoch 17/25\n",
            "631/631 [==============================] - 154s 244ms/step - loss: 0.3096 - accuracy: 0.8676 - val_loss: 0.4330 - val_accuracy: 0.8106\n",
            "Epoch 18/25\n",
            "631/631 [==============================] - 154s 243ms/step - loss: 0.2926 - accuracy: 0.8779 - val_loss: 0.4564 - val_accuracy: 0.8070\n",
            "Epoch 19/25\n",
            "631/631 [==============================] - 146s 231ms/step - loss: 0.2852 - accuracy: 0.8756 - val_loss: 0.4734 - val_accuracy: 0.8040\n",
            "Epoch 20/25\n",
            "631/631 [==============================] - 139s 221ms/step - loss: 0.2742 - accuracy: 0.8842 - val_loss: 0.4454 - val_accuracy: 0.8168\n",
            "Epoch 21/25\n",
            "631/631 [==============================] - 144s 229ms/step - loss: 0.2625 - accuracy: 0.8884 - val_loss: 0.5005 - val_accuracy: 0.8036\n",
            "Epoch 22/25\n",
            "631/631 [==============================] - 140s 222ms/step - loss: 0.2593 - accuracy: 0.8883 - val_loss: 0.5196 - val_accuracy: 0.8002\n",
            "Epoch 23/25\n",
            "631/631 [==============================] - 139s 221ms/step - loss: 0.2420 - accuracy: 0.9004 - val_loss: 0.4777 - val_accuracy: 0.8108\n",
            "Epoch 24/25\n",
            "631/631 [==============================] - 144s 229ms/step - loss: 0.2341 - accuracy: 0.9014 - val_loss: 0.5617 - val_accuracy: 0.7928\n",
            "Epoch 25/25\n",
            "631/631 [==============================] - 144s 229ms/step - loss: 0.2320 - accuracy: 0.9047 - val_loss: 0.5191 - val_accuracy: 0.8116\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fbfe69b99f0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO2vGSuuY0ya"
      },
      "source": [
        "Load the image to make a single prediction and make sure it is the same size as the training images. Convert the image to an array, and use expand_dims() method to add a dimension to the batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4l6sCJP-L32A",
        "outputId": "0d28c2f8-d8cd-48c2-85b2-ae2d25bb2262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 19ms/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image = image.load_img('/content/drive/My Drive/CNN/test/cats/cat.128.jpg', target_size = (64, 64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = cnn.predict(test_image)\n",
        "training_set.class_indices\n",
        "if result[0][0] == 1:\n",
        "  prediction = 'dog'\n",
        "else:\n",
        "  prediction = 'cat'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVIL7saMPNeD",
        "outputId": "975194a7-f3a5-4556-8d52-59331ea32b34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cat\n"
          ]
        }
      ],
      "source": [
        "print(prediction)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
